--- etc/slurm.conf.example.orig	2021-09-21 13:21:07 UTC
+++ etc/slurm.conf.example
@@ -9,7 +9,7 @@
 # See the slurm.conf man page for more information.
 #
 ClusterName=cluster
-SlurmctldHost=linux0
+SlurmctldHost=head
 #SlurmctldHost=
 #
 #DisableRootJobs=NO
@@ -41,13 +41,13 @@ ProctrackType=proctrack/cgroup
 #PrologFlags=
 #PrologSlurmctld=
 #PropagatePrioProcess=0
-#PropagateResourceLimits=
+PropagateResourceLimits=NONE
 #PropagateResourceLimitsExcept=
 #RebootProgram=
 ReturnToService=1
-SlurmctldPidFile=/var/run/slurmctld.pid
+SlurmctldPidFile=/var/run/slurm/slurmctld.pid
 SlurmctldPort=6817
-SlurmdPidFile=/var/run/slurmd.pid
+SlurmdPidFile=/var/run/slurm/slurmd.pid
 SlurmdPort=6818
 SlurmdSpoolDir=/var/spool/slurmd
 SlurmUser=slurm
@@ -58,6 +58,8 @@ StateSaveLocation=/var/spool/slurmctld
 SwitchType=switch/none
 #TaskEpilog=
 TaskPlugin=task/affinity
+# For debugging: TaskPluginParam=cores,verbose
+TaskPluginParam=cores
 #TaskProlog=
 #TopologyPlugin=topology/tree
 #TmpFS=/tmp
@@ -88,12 +90,12 @@ Waittime=0
 #
 #
 # SCHEDULING
-#DefMemPerCPU=0
+DefMemPerCPU=256
 #MaxMemPerCPU=0
 #SchedulerTimeSlice=30
 SchedulerType=sched/backfill
 SelectType=select/cons_tres
-SelectTypeParameters=CR_Core
+SelectTypeParameters=CR_Core_Memory
 #
 #
 # JOB PRIORITY
@@ -129,9 +131,9 @@ JobCompType=jobcomp/none
 JobAcctGatherFrequency=30
 JobAcctGatherType=jobacct_gather/none
 SlurmctldDebug=info
-SlurmctldLogFile=/var/log/slurmctld.log
+SlurmctldLogFile=/var/log/slurm/slurmctld.log
 SlurmdDebug=info
-SlurmdLogFile=/var/log/slurmd.log
+SlurmdLogFile=/var/log/slurm/slurmd.log
 #SlurmSchedLogFile=
 #SlurmSchedLogLevel=
 #DebugFlags=
@@ -148,7 +150,39 @@ SlurmdLogFile=/var/log/slurmd.log
 #SuspendRate=
 #SuspendTime=
 #
+############################################################################
+# Enable power saving if remote IPMI power-on is available on compute nodes.
+# If unavailable on some nodes, list them in SuspendExcNodes.
+# SlurmUser must be a member of operator and wheel and have a valid
+# login shell in order to execute shutdown on compute nodes.
+# If you prefer to control power manually, see the following scripts
+# from the SPCM port:
 #
+#       auto-ipmi-remote-power
+#       cluster-power-saver
+#       cluster-power-waster
+#       cluster-ipmi-power-on
+############################################################################
+
+# SuspendProgram=/usr/local/etc/spcm/slurm-node-suspend
+# SuspendTime should be >= SuspendTimeout + ResumeTimeout.
+# SuspendTime=600
+# SuspendTimeout=60
+#
+# ResumeProgram=/usr/local/etc/spcm/slurm-node-resume
+# ResumeTimeout=300
+# BatchStartTimeout=300
+#
+# Exempt compute nodes that double as file servers or don't have IPMI
+# remote power-on enabled.
+# SuspendExcNodes=compute-001
+
 # COMPUTE NODES
-NodeName=linux[1-32] CPUs=1 State=UNKNOWN
-PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP
+# Set RealMemory < avail memory in /var/run/dmesg.boot
+# Note that it may change slightly following freebsd-update
+# Minimal node specs usable with VMs for testing
+NodeName=compute-[001-002] Sockets=1 CoresPerSocket=1 RealMemory=1000 State=UNKNOWN
+# NodeName=compute-256g-[001-002] Sockets=2 CoresPerSocket=6 RealMemory=250000 State=UNKNOWN
+# PartitionName=debug Nodes=ALL Default=NO MaxTime=INFINITE State=UP
+PartitionName=batch Nodes=compute-[001-002] Default=YES MaxTime=INFINITE State=UP
+# PartitionName=256g Nodes=compute-256g-[001-002] Default=NO MaxTime=INFINITE State=UP
